# MEGAISurv analysis workflow

[![Project Status: WIP – Initial development is in progress, but there has not yet been a stable, usable release suitable for the public.](https://www.repostatus.org/badges/latest/wip.svg)](https://www.repostatus.org/#wip) [![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)

**ME**ta**G**enome-informed **A**ntimicrobial res**i**stance **Surv**eillance:
Harnessing long-read sequencing for an analytical, indicator and risk assessment
framework.

# Index

1. [Workflow description](#workflow-description)
    - [microbiota profiling](#microbiota-profiling)
    - [future ideas](#future-ideas)
2. [Project (file) organisation](#project-organisation)
3. [Licence](#licence)
4. [Citation](#citation)

# Workflow description

Simple description:

1. Metagenomes are assembled using [metaFlye](https://github.com/mikolmogorov/Flye) (version 2.9.2)

2. Assembled contigs are taxonomically classified using [Centrifuger](https://github.com/mourisl/centrifuger) (version 1.0.6)

3. Antibiotic resistance genes are identified using [KMA](https://github.com/genomicepidemiology/kma) (version 1.4.2)

4. Antibiotic resistance genes are masked using [bedtools](https://bedtools.readthedocs.io/en/latest/index.html) (function `maskFastaFromBed`; version 2.31.1)

## Microbiota profiling:

### Taxonomic assignment and quantification

For the taxonomic classification of the metagenomes (also known as microbiota profiling),
we are using the metagenomic assemblies generated by Flye and classify them with
Centrifuger. As Centrifuger expects reads rather than contigs, the relative
abundances need to be manually adjusted. To do this, we use the contig length 
and depth of coverage as reported in the assembly statistics provided by Flye. 
File `assembly_info.txt`. With this we calculate the total number of bases
assigned to each taxon and from that we calculate the percentage assigned to
each species.

Also, Centrifuger does not report taxon names per read/contig automatically.
Instead, it provides the tax IDs as reported in the NCBI taxonomy database.
To translate these to species names and complete taxonomic lineages, we use
[TaxonKit](https://bioinf.shenwei.me/taxonkit) (version 0.18.0) with the
NCBI taxdump (ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz) 
downloaded on 26 February 2025.

The practical implementation of this workflow is described in the
[`Snakefile`](Snakefile) and is as follows:

 1. Classify contigs using Centrifuger with default parameters and the 'cfr_hpv+sarscov2' database
(Which is available on Zenodo: https://zenodo.org/records/10023239)
 
 2. Attach species and taxon lineage names using TaxonKit
 
 3. Quantify by combining Centrifuger's output and Flye's assembly statistics
in a custom R script. (I.e., for each contig, multiply its length with its depth
to represent 'total_bases', then calculate percentages per contig and per taxon
based on these total_bases.)

### TaxonKit user note

Extra note on using TaxonKit: after downloading the taxdump tarball itself, e.g.:

` $ wget ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz`

it can be useful to check if it is complete by comparing the md5 checksum:

```bash
# Download MD5 checksum
wget ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz.md5
# Check file integrity
md5sum -c taxdump.tax.gz.md5

# Then extract the tarball
tar -xzf taxdump.tar.gz
```

TaxonKit relies on the files: `names.dmp`, `nodes.dmp`, `delnodes.dmp` and `merged.dmp`.
Copy or move them to the `.taxonkit` directory in your home folder (which should be automatically
generated when you install taxdump) to be able to use `taxonkit`. E.g.:

```bash
mv *.dmp ~/.taxonkit/
```

## Future ideas

- change the order: 1) screen ARG, 2) mask, 3) assign taxonomy
(Because taxonomic classification can be influenced by ARG!)
- rename subdirectories in Snakefile to be more descriptive of contents;
e.g., 'flye' --> 'assemblies'
- add quality control tools: fastplong (reads), metaQUAST (assemblies), multiQC (combine in one report)
- add R? scripts to combine relevant output from different tools
- include downstream processing scripts (RMarkdown) for statistical analyses
and visualisation

Actually, after some more testing and looking at the results, I think the whole workflow needs an overhaul:
1. quality-control reads with fastplong
2. assemble high-quality reads (faster and fewer artifacts?)
3. filter contigs to minimum length of 2-3x average read length?
4. screen ARGs and mask
5. classify all contigs (test Centrifuger (with extended DB?), CAT and sourmash)
6. summarise in neat table/report(?)

# Project organisation

```
.
├── CITATION.cff
├── LICENSE
├── README.md
├── Snakefile          <- Python-based workflow description
├── bin                <- Code and programs used in this project/experiment
├── config             <- Configuration of Snakemake workflow
├── data               <- All project data, divided in subfolders
│   ├── processed      <- Final data, used for visualisation (e.g. tables)
│   ├── raw            <- Raw data, original, should not be modified (e.g. fastq files)
│   └── tmp            <- Intermediate data, derived from the raw data, but not yet ready for visualisation
├── doc                <- Project documentation, notes and experiment records
├── envs               <- Conda environments necessary to run the project/experiment
├── log                <- Log files from programs
└── results            <- Figures or reports generated from processed data
```

# Licence

This project is licensed under the terms of the [New BSD licence](LICENSE).

# Citation

Please cite this project as described in the [citation file](CITATION.cff).
