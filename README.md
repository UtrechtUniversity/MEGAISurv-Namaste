# MEGAISurv analysis workflow

[![Project Status: WIP – Initial development is in progress, but there has not yet been a stable, usable release suitable for the public.](https://www.repostatus.org/badges/latest/wip.svg)](https://www.repostatus.org/#wip)

Automated bioinformatics workflow to process and analyse long-read (minION)
metagenomes, screen for the presence of antibiotic resistance genes and classify
contigs taxonomically while masking the resistance genes.

# Index

1. [Workflow description](#workflow-description)
    - [microbiota profiling](#microbiota-profiling)
    - [future ideas](#future-ideas)
2. [Project (file) organisation](#project-organisation)
3. [Licence](#licence)
4. [Citation](#citation)

# Workflow description

Simple description:

1. Metagenomes are assembled using [metaFlye](https://github.com/mikolmogorov/Flye) (version 2.9.2)

2. Assembled contigs are taxonomically classified using [Centrifuger](https://github.com/mourisl/centrifuger) (version 1.0.6)

3. Antibiotic resistance genes are identified using [KMA](https://github.com/genomicepidemiology/kma) (version 1.4.2)

4. Antibiotic resistance genes are masked using [bedtools](https://bedtools.readthedocs.io/en/latest/index.html) (function `maskFastaFromBed`; version 2.31.1)

## Microbiota profiling:

### Taxonomic assignment and quantification

For the taxonomic classification of the metagenomes (also known as microbiota profiling),
we are using the metagenomic assemblies generated by Flye and classify them with
Centrifuger. As Centrifuger expects reads rather than contigs, the relative
abundances need to be manually adjusted. To do this, we use the contig length 
and depth of coverage as reported in the assembly statistics provided by Flye. 
File `assembly_info.txt`. With this we calculate the total number of bases
assigned to each taxon and from that we calculate the percentage assigned to
each species.

Also, Centrifuger does not report taxon names per read/contig automatically.
Instead, it provides the tax IDs as reported in the NCBI taxonomy database.
To translate these to species names and complete taxonomic lineages, we use
[TaxonKit](https://bioinf.shenwei.me/taxonkit) (version 0.18.0) with the
NCBI taxdump (ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz) 
downloaded on 26 February 2025.

The practical implementation of this workflow is described in the
[`Snakefile`](Snakefile) and is as follows:

 1. Classify contigs using Centrifuger with default parameters and the 'cfr_hpv+sarscov2' database
(Which is available on Zenodo: https://zenodo.org/records/10023239)
 
 2. Attach species and taxon lineage names using TaxonKit
 
 3. Quantify by combining Centrifuger's output and Flye's assembly statistics
in a custom R script. (I.e., for each contig, multiply its length with its depth
to represent 'total_bases', then calculate percentages per contig and per taxon
based on these total_bases.)

### TaxonKit user note

Extra note on using TaxonKit: after downloading the taxdump tarball itself, e.g.:

` $ wget ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz`

it can be useful to check if it is complete by comparing the md5 checksum:

```bash
# Download MD5 checksum
wget ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz.md5
# Check file integrity
md5sum -c taxdump.tax.gz.md5

# Then extract the tarball
tar -xzf taxdump.tar.gz
```

TaxonKit relies on the files: `names.dmp`, `nodes.dmp`, `delnodes.dmp` and `merged.dmp`.
Copy or move them to the `.taxonkit` directory in your home folder (which should be automatically
generated when you install taxdump) to be able to use `taxonkit`. E.g.:

```bash
mv *.dmp ~/.taxonkit/
```

## Future ideas

 - change the order: 1) screen ARG, 2) mask, 3) assign taxonomy
 - add quality control tools: fastp?, metaQUAST, multiQC
 - add R? scripts to combine relevant output from different tools

*Note: MinION-specific QC tools rely on the `sequencing_summary.txt` file generated by the basecalling program, so not the fastq files!*


# Project organisation

```
.
├── CITATION.cff
├── LICENSE
├── README.md
├── Snakefile          <- Python-based workflow description
├── bin                <- Code and programs used in this project/experiment
├── config             <- Configuration of Snakemake workflow
├── data               <- All project data, divided in subfolders
│   ├── processed      <- Final data, used for visualisation (e.g. tables)
│   ├── raw            <- Raw data, original, should not be modified (e.g. fastq files)
│   └── tmp            <- Intermediate data, derived from the raw data, but not yet ready for visualisation
├── doc                <- Project documentation, notes and experiment records
├── envs               <- Conda environments necessary to run the project/experiment
├── log                <- Log files from programs
└── results            <- Figures or reports generated from processed data
```

# Licence

_to do_

# Citation

_to do_

