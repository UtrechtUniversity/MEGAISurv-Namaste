# MEGAISurv Namaste :pray:

[![Project Status: WIP – Initial development is in progress, but there has not yet been a stable, usable release suitable for the public.](https://www.repostatus.org/badges/latest/wip.svg)](https://www.repostatus.org/#wip) [![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)

**Namaste**: Nanopore Metagenomics antibiotic Resistance and Taxonomy Screening
for the project MEGAISurv:

**ME**ta**G**enome-informed **A**ntimicrobial res**i**stance **Surv**eillance:
Harnessing long-read sequencing for an analytical, indicator and risk assessment
framework.

## Index

1. [Workflow description](#workflow-description)
    - [microbiota profiling](#microbiota-profiling)
    - [future ideas](#future-ideas)
2. [Project (file) organisation](#project-organisation)
3. [Licence](#licence)
4. [Citation](#citation)

## Workflow description

Simple description:

1. Metagenomic reads are preprocessed using [fastplong](https://github.com/OpenGene/fastplong) (version 0.2.2)

2. High-quality reads are assembled using [metaFlye](https://github.com/mikolmogorov/Flye) (version 2.9.2)

3. Antibiotic resistance genes are identified using [KMA](https://github.com/genomicepidemiology/kma) (version 1.4.2)

4. Resistance genes are masked using [BEDtools](https://bedtools.readthedocs.io/en/latest/index.html) (function `maskFastaFromBed`; version 2.31.1)

5. Assembled and masked contigs are taxonomically classified using [Centrifuger](https://github.com/mourisl/centrifuger) (version 1.0.6)

### Microbiota profiling

#### Taxonomic assignment and quantification

For the taxonomic classification of the metagenomes (also known as microbiota profiling),
we are using the metagenomic assemblies generated by Flye and classify them with
Centrifuger. As Centrifuger expects reads rather than contigs, the relative
abundances need to be manually adjusted. To do this, we use the contig length
and depth of coverage as reported in the assembly statistics provided by Flye.
File `assembly_info.txt`. With this we calculate the total number of bases
assigned to each taxon and from that we calculate the percentage assigned to
each species.

Also, Centrifuger does not report taxon names per read/contig automatically.
Instead, it provides the tax IDs as reported in the NCBI taxonomy database.
To translate these to species names and complete taxonomic lineages, we use
[TaxonKit](https://bioinf.shenwei.me/taxonkit) (version 0.18.0) with the
NCBI taxdump (ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz)
downloaded on 26 February 2025.

The practical implementation of this workflow is described in the
[`Snakefile`](Snakefile) and is as follows:

1. Classify contigs using Centrifuger with default parameters and the 'cfr_hpv+sarscov2' database
(Which is available on [Zenodo](https://zenodo.org/records/10023239))

2. Attach species and taxon lineage names using TaxonKit

3. Quantify by combining Centrifuger's output and Flye's assembly statistics
in a custom R script. (I.e., for each contig, multiply its length with its depth
to represent 'total_bases', then calculate percentages per contig and per taxon
based on these total_bases.)

#### TaxonKit user note

Extra note on using TaxonKit: after downloading the taxdump tarball itself, e.g.:

`wget ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz`

it can be useful to check if it is complete by comparing the md5 checksum:

```bash
# Download MD5 checksum
wget ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz.md5
# Check file integrity
md5sum -c taxdump.tax.gz.md5

# Then extract the tarball
tar -xzf taxdump.tar.gz
```

TaxonKit relies on the files: `names.dmp`, `nodes.dmp`, `delnodes.dmp` and `merged.dmp`.
Copy or move them to the `.taxonkit` directory in your home folder (which should be automatically
generated when you install taxdump) to be able to use `taxonkit`. E.g.:

```bash
mv *.dmp ~/.taxonkit/
```

## Future ideas

- Summarise the final output in one neat table

- Include downstream processing scripts (RMarkdown) for statistical analyses
and visualisation

- Test alternative contig classification databases and tools

- Filter contigs to minimum length of 2-3x average read length?

- **Write/extend documentation of the whole workflow and interesting findings**

## Project organisation

```bash
.
├── CITATION.cff
├── LICENSE
├── README.md
├── Snakefile          <- Python-based workflow description
├── bin                <- Code and programs used in this project/experiment
├── config             <- Configuration of Snakemake workflow
├── data               <- All project data, divided in subfolders
│   ├── processed      <- Final data, used for visualisation (e.g. tables)
│   ├── raw            <- Raw data, original, should not be modified (e.g. fastq files)
│   └── tmp            <- Intermediate data, derived from the raw data, but not yet ready for visualisation
├── doc                <- Project documentation, notes and experiment records
├── envs               <- Conda environments necessary to run the project/experiment
├── log                <- Log files from programs
└── results            <- Figures or reports generated from processed data
```

## Licence

This project is licensed under the terms of the [New BSD licence](LICENSE).

## Citation

Please cite this project as described in the [citation file](CITATION.cff).
